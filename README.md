# In-context learning Secrets

Relevant papers:

[In-context Learning and Induction Heads](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html): This paper talks about an emerging phenomena in training transformers called induction heads. As the naming indicates, an induction heads refers to a special type of attention heads that is able to do "induction" reasoning.
[In-Context Learning Creates Task Vectors](https://arxiv.org/abs/2310.15916)
[Transformers learn in-context by gradient descent](http://arxiv.org/abs/2212.07677)
[Transformers Learn Higher-Order Optimization Methods for In-Context Learning: A Study with Linear Models](https://arxiv.org/abs/2210.05675)
[Looped Transformers as Programmable Computers](https://arxiv.org/abs/2301.13196)
[SUCCESSOR HEADS: RECURRING, INTERPRETABLE ATTENTION HEADS IN THE WILD](https://arxiv.org/pdf/2312.09230)
[Transformers generalize differently from information stored in context vs in weights](http://arxiv.org/abs/2210.05675)
[Many-Shot In-Context Learning](http://arxiv.org/abs/2404.11018)
[Can language models learn from explanations in context?](https://arxiv.org/abs/2204.02329)
[Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?](http://arxiv.org/abs/2202.12837)



