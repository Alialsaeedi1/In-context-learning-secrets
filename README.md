# In-context learning Secrets

Relevant papers:

1. [In-context Learning and Induction Heads](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html): This paper talks about an emerging phenomena in training transformers called induction heads. As the naming indicates, an induction heads refers to a special type of attention heads that is able to do "induction" reasoning.

2. [In-Context Learning Creates Task Vectors](https://arxiv.org/abs/2310.15916)

3. [Transformers learn in-context by gradient descent](http://arxiv.org/abs/2212.07677)

4. [Transformers Learn Higher-Order Optimization Methods for In-Context Learning: A Study with Linear Models](https://arxiv.org/abs/2210.05675)

5. [Looped Transformers as Programmable Computers](https://arxiv.org/abs/2301.13196)

6. [SUCCESSOR HEADS: RECURRING, INTERPRETABLE ATTENTION HEADS IN THE WILD](https://arxiv.org/pdf/2312.09230)

7. [Transformers generalize differently from information stored in context vs in weights](http://arxiv.org/abs/2210.05675)

8. [Many-Shot In-Context Learning](http://arxiv.org/abs/2404.11018)

9. [Can language models learn from explanations in context?](https://arxiv.org/abs/2204.02329)

10. [Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?](http://arxiv.org/abs/2202.12837)

11. [The Developmental Landscape of In-Context Learning](https://arxiv.org/abs/2402.02364)

12. [IN-CONTEXT LANGUAGE LEARNING:ARCHITECTURES AND ALGORITHMS](https://arxiv.org/pdf/2401.12973)

13. [https://arxiv.org/abs/2402.11004](The Evolution of Statistical Induction Heads: In-Context Learning Markov Chains)

14. [https://www.lesswrong.com/posts/j6s9H9SHrEhEfuJnq/causal-scrubbing-results-on-induction-heads](Causal scrubbing: results on induction heads)
